"""
This file provides a class that can learn APATE graph-based features. Implementation based on:

"APATE: A novel approach for automated credit card transaction fraud detection using network-
based extensions" by VÃ©ronique Van Vlasselaer, CristiÃ¡n Bravo, Olivier Caelen, Tina
Eliassa-Rad, Leman Akoglu, Monique Snoeck, and Bart Baesens

"A graph-based, semi-supervised, credit card fraud detection system" by Bertand Lebichot, Fabian Braun,
Olivier Caelen, and Marco Saerens

Notation in the code (variable names) primarily based on notation used in the second paper of the two above.




The intuition of this feature engineering procedure is to construct a graph (using training data) where Merchants,
Card Holders and Transactions are nodes. This is a tripartite graph, where there are only edges between Merchants and
Transactions, and edges between Card Holders and Transactions (so no edges between two nodes of the same type, and no
edges directly between Card Holders and Merchants). Every transaction has exactly two edges; one with the Merchant,
and one with the Card Holder corresponding to that transaction.

Through a convergence procedure, ''influence'' of fraudulent transactions is spread throughout the network. This
influence will be higher in parts of the network where there is a high concentration of Merchants and/or Card
Holders that are frequently involved in fraudulent transactions.

After running the convergence procedure using training data, an object of this class can be used to provide various
scores (new features) for new transactions. For new transactions where the Card Holder as well as the Merchant
have never been observed in training data, these scores will always simply be 0 because we have no information on them.
For new transactions where the Card Holder and/or the Merchant have already previously been seen in the training data,
this procedure can provide useful scores related to how ''risky'' they are.

@author: Dennis Soemers
"""

import math
from datetime import datetime
from scipy.sparse import coo_matrix
from scipy.sparse import diags


class ApateGraphFeatures:

    def __init__(self, training_data, convergence_threshold=0.0000001, alpha=0.85):
        """
        Constructs an ''APATE Graph Features'' object based on a given set of training data.
        This will run the convergence procedure on the entire training data, which can easily
        take several minutes for large datasets.

        Once this construction has finished, the object can be used at later points in time to
        generate new features for new transactions outside of the original training data.

        In the real-world setting described in the two papers this implementation is based on,
        this convergence procedure would be re-run on the latest available training data every day
        at midnight. So, for experiments simulating long-term scenarios, it is recommended to reconstruct
        new objects of this class, with new training data, at regular intervals.

        :param training_data:
            The dataset to use for generating the network and running the convergence procedure.
            Expected to be in the format generated by the data/preprocess_data_raw.py script.

        :param convergence_threshold:
            Threshold used to determine whether or not the scores have converged in the convergence
            procedure. When an iteration does not change any score by an amount that exceeds this value,
            the convergence procedure will be terminated.
        :param alpha:
            This parameter can be interpreted as the probability to continue a walk (instead of restarting)
            in a Random Walk with Restart Procedure. Both of the papers this implementation is based only
            mention using a value of 0.85
        """
        # print(str(datetime.now()), ": Init APATE Graph Features...")

        # init nodes type names
        self.card_col_name = "card_number"
        self.merchant_col_name = "merchant"
        self.email_col_name = "email_address"
        self.ip_col_name = "ip_address"

        # label propagation parameters
        self.CONVERGENCE_THRESHOLD = convergence_threshold
        self.ALPHA = alpha

        self.cards_dict = {}
        self.merchants_dict = {}
        self.ip_dict = {}
        self.email_dict = {}

        self.next_card_idx = 0
        self.next_merchant_idx = 0
        self.next_ip_idx = 0
        self.next_email_idx = 0

        cards = list(training_data[self.card_col_name])
        merchants = list(training_data[self.merchant_col_name])
        ip_list = list(training_data[self.ip_col_name])
        email_list = list(training_data[self.email_col_name])

        self.num_t = len(cards)  # number of transactions
        self.num_c = len(training_data[self.card_col_name].unique())  # number of cards
        self.num_m = len(training_data[self.merchant_col_name].unique())  # number of merchants
        self.num_e = len(training_data[self.email_col_name].unique())  # number of email
        self.num_ip = len(training_data[self.ip_col_name].unique())  # number of ip

        # these four are kinda like enums, but not really enums because I got the impression enums are slow in python
        self.NO_INTERVAL = 0
        self.SHORT_TERM = 1
        self.MEDIUM_TERM = 2
        self.LONG_TERM = 3

        # local versions of them, faster access than the class members within this function
        NO_INTERVAL = self.NO_INTERVAL
        SHORT_TERM = self.SHORT_TERM
        MEDIUM_TERM = self.MEDIUM_TERM
        LONG_TERM = self.LONG_TERM

        # compute A^{tri} adjacency matrices of tripartite graph with various levels of decay (Q^{tri})
        rows_A = []
        cols_A = []
        entries_A = []
        entries_A_short = []
        entries_A_medium = []
        entries_A_long = []

        # at the same time, we'll also compute the starting vectors r_0 with various levels of decay
        rows_r0 = []
        cols_r0 = []
        entries_r0 = []
        entries_r0_short = []
        entries_r0_medium = []
        entries_r0_long = []

        '''
        We'll use the latest date in the training data as ''test date''. Influence of older transactions
        will be decayed with respect to this date. I suppose that, ideally, we'd want to always decay with
        respect to a single test instance that we're generating features for. That would be completely infeasible
        computationally though, since it would mean re-running the entire convergence procedure over and over
        again for every single test instance.

        Another reasonable alternative could be to use the first date of a test set. It is not clear which timepoint
        was used in the two papers this implementation is based on.
        '''
        test_date = training_data.Global_Date.max()

        t = 0  # transaction index

        # evaluate a bunch of function references so we don't keep re-evaluating them many times in loop
        get_card_idx = self.get_card_idx
        get_merchant_idx = self.get_merchant_idx
        get_email_idx = self.get_email_idx
        get_ip_idx = self.get_ip_idx
        compute_A_entry = self.compute_A_entry

        rows_extend = rows_A.extend
        cols_extend = cols_A.extend
        A_extend = entries_A.extend
        A_extend_short = entries_A_short.extend
        A_extend_medium = entries_A_medium.extend
        A_extend_long = entries_A_long.extend

        rows_r0_append = rows_r0.append
        cols_r0_append = cols_r0.append
        entries_r0_append = entries_r0.append
        entries_r0_short_append = entries_r0_short.append
        entries_r0_medium_append = entries_r0_medium.append
        entries_r0_long_append = entries_r0_long.append

        # and save these in local variables
        num_t = self.num_t  # total number of transactions
        num_c = self.num_c  # total number of cards
        num_m = self.num_m  # total number of merchants
        num_e = self.num_e  # total number of email
        num_ip = self.num_ip  # total number of ip

        for row in training_data.itertuples():
            # for each transaction entry, we get index for each node types
            c = get_card_idx(cards[t])
            m = get_merchant_idx(merchants[t])
            e = get_email_idx(email_list[t])
            ip = get_ip_idx(ip_list[t])

            # compute entries in adjacency matrix with different implementations of decay
            row_global_date = row.Global_Date

            # we calculate the recency of the transactions based on different window
            short_term_entry = compute_A_entry(row_global_date, test_date, gamma=0.03, interval=SHORT_TERM)
            medium_term_entry = compute_A_entry(row_global_date, test_date, gamma=0.004, interval=MEDIUM_TERM)
            long_term_entry = compute_A_entry(row_global_date, test_date, gamma=0.0001, interval=LONG_TERM)

            '''
                entry in A_{t * c}, entry in A_{t * m},     entry in A_{c * t}, entry in A_{m * t}
            '''

            '''
                num_t # total number of transactions
                num_c # total number of cards
                num_m # total number of merchants
                num_ip # total number of ip
                num_e # total number of email
                t # transaction index
                c # card index
                m # merchant index
                ip # ip index
                e # email index
                
                Q^{tri} -> 
            '''
            rows_extend(
                [t, t, t, t, c + num_t, m + num_c + num_t, ip + num_m + num_c + num_t,
                 e + num_m + num_c + num_t + num_ip]
            )
            cols_extend(
                [c + num_t, m + num_c + num_t, ip + num_m + num_c + num_t, e + num_m + num_c + num_t + num_ip, t, t, t,
                 t]
            )
            A_extend(
                [1.0] * 8
                # consider every  transaction is recent (no recency considerations)
            )
            A_extend_short(
                [short_term_entry] * 8
            )
            A_extend_medium(
                [medium_term_entry] * 8
            )
            A_extend_long(
                [long_term_entry] * 8
            )

            # init exposure score as 1 for fraudulent transaction nodes
            if row.Target == 1:
                # fraudulent transaction, add entries to r_0 vectors
                rows_r0_append(t)
                cols_r0_append(0.0)
                entries_r0_append(1.0)
                entries_r0_short_append(short_term_entry)
                entries_r0_medium_append(medium_term_entry)
                entries_r0_long_append(long_term_entry)

            # increment transaction counter
            t += 1

        # print(str(datetime.now()), ": Finished computing entries...")

        # we'll use a sparse matrix. May not yet be necessary for original dataset, but will be necessary
        # for larger datasets from simulator
        dim = num_t + num_c + num_m + num_ip + num_e
        A_tri_shape = (dim, dim)

        # csr Compressed Sparse Row format
        self.A_tri = coo_matrix((entries_A, (rows_A, cols_A)), shape=A_tri_shape).tocsr()
        self.A_tri_short = coo_matrix((entries_A_short, (rows_A, cols_A)), shape=A_tri_shape).tocsr()
        self.A_tri_medium = coo_matrix((entries_A_medium, (rows_A, cols_A)), shape=A_tri_shape).tocsr()
        self.A_tri_long = coo_matrix((entries_A_long, (rows_A, cols_A)), shape=A_tri_shape).tocsr()

        # print(str(datetime.now()), ": Finished creating matrices...")

        self.r0 = coo_matrix((entries_r0, (rows_r0, cols_r0)), shape=(dim, 1)).tocsc()
        self.r0_short = coo_matrix((entries_r0_short, (rows_r0, cols_r0)), shape=(dim, 1)).tocsc()
        self.r0_medium = coo_matrix((entries_r0_medium, (rows_r0, cols_r0)), shape=(dim, 1)).tocsc()
        self.r0_long = coo_matrix((entries_r0_long, (rows_r0, cols_r0)), shape=(dim, 1)).tocsc()

        # print(str(datetime.now()), ": Finished creating vectors...")

        # normalize rows to sum up to 1.0 for all the A_tri matrices (first paper says normalize columns,
        # but we structured the matrix as in second paper, so we need to normalize rows)
        normalize_rows = self.normalize_rows
        self.A_tri = normalize_rows(self.A_tri)
        self.A_tri_short = normalize_rows(self.A_tri_short)
        self.A_tri_medium = normalize_rows(self.A_tri_medium)
        self.A_tri_long = normalize_rows(self.A_tri_long)

        # print(str(datetime.now()), ": Finished normalizing matrices...")

        # normalize r0 vectors to sum up to 1.0 (this is described in the first article, but not the second)
        self.r0 = self.r0 / self.r0.sum()
        self.r0_short = self.r0_short / self.r0_short.sum()
        self.r0_medium = self.r0_medium / self.r0_medium.sum()
        self.r0_long = self.r0_long / self.r0_long.sum()

        # print(str(datetime.now()), ": Finished normalizing vectors...")

        # run convergence procedure for the four different levels of decay
        converge = self.converge
        self.rkc = converge(self.r0, self.A_tri,
                            alpha=self.ALPHA, convergence_threshold=self.CONVERGENCE_THRESHOLD)
        # print(str(datetime.now()), ": Finished convergence without decay...")

        self.rkc_short = converge(self.r0_short, self.A_tri_short,
                                  alpha=self.ALPHA, convergence_threshold=self.CONVERGENCE_THRESHOLD)
        # print(str(datetime.now()), ": Finished convergence with short-term decay...")

        self.rkc_medium = converge(self.r0_medium, self.A_tri_medium,
                                   alpha=self.ALPHA, convergence_threshold=self.CONVERGENCE_THRESHOLD)
        # print(str(datetime.now()), ": Finished convergence with medium-term decay...")

        self.rkc_long = converge(self.r0_long, self.A_tri_long,
                                 alpha=self.ALPHA, convergence_threshold=self.CONVERGENCE_THRESHOLD)
        # print(str(datetime.now()), ": Finished convergence with long-term decay...")

        self.A_tri.eliminate_zeros()
        self.A_tri_short.eliminate_zeros()
        self.A_tri_medium.eliminate_zeros()
        self.A_tri_long.eliminate_zeros()

        self.rkc.eliminate_zeros()
        self.rkc_short.eliminate_zeros()
        self.rkc_medium.eliminate_zeros()
        self.rkc_long.eliminate_zeros()

        # we'll compute the following things once when we need them
        self.rkc_avg_card = None
        self.rkc_short_avg_card = None
        self.rkc_medium_avg_card = None
        self.rkc_long_avg_card = None

        self.rkc_avg_mer = None
        self.rkc_short_avg_mer = None
        self.rkc_medium_avg_mer = None
        self.rkc_long_avg_mer = None

        self.rkc_avg_ip = None
        self.rkc_short_avg_ip = None
        self.rkc_medium_avg_ip = None
        self.rkc_long_avg_ip = None

        self.rkc_avg_email = None
        self.rkc_short_avg_email = None
        self.rkc_medium_avg_email = None
        self.rkc_long_avg_email = None

        self.A_tri_avg_card = None
        self.A_tri_short_avg_card = None
        self.A_tri_medium_avg_card = None
        self.A_tri_long_avg_card = None

        self.A_tri_avg_mer = None
        self.A_tri_short_avg_mer = None
        self.A_tri_medium_avg_mer = None
        self.A_tri_long_avg_mer = None

        self.A_tri_avg_ip = None
        self.A_tri_short_avg_ip = None
        self.A_tri_medium_avg_ip = None
        self.A_tri_long_avg_ip = None

        self.A_tri_avg_email = None
        self.A_tri_short_avg_email = None
        self.A_tri_medium_avg_email = None
        self.A_tri_long_avg_email = None

    def add_graph_features(self, test_data):
        """
        Adds 12 new features, based on the constructed network, to the entire given test dataset. The features
        added are:

        - Four risk scores for the Card Holder, with (1) no, (2) short-term, (3) medium-term, or (4) long-term decay.
        - Four risk scores for the Merchant, with (1) no, (2) short-term, (3) medium-term, or (4) long-term decay.
        - Four risk scores for the Transaction, with (1) no, (2) short-term, (3) medium-term, or (4) long-term decay.

        :param test_data:
            A pandas dataframe containing all the transactions we want to add new features to.
        """


        # add four scores for ip (four levels of decay)
        get_ip_score = self.get_ip_score
        test_data["IPScore"] = test_data.parallel_apply(lambda row: get_ip_score(row, self.NO_INTERVAL), axis=1)
        test_data["IPScore_ST"] = test_data.parallel_apply(lambda row: get_ip_score(row, self.SHORT_TERM), axis=1)
        test_data["IPScore_MT"] = test_data.parallel_apply(lambda row: get_ip_score(row, self.MEDIUM_TERM), axis=1)
        test_data["IPScore_LT"] = test_data.parallel_apply(lambda row: get_ip_score(row, self.LONG_TERM), axis=1)
        print(str(datetime.now()), ": Finished computing IP features...")

        # add four scores for email (four levels of decay)
        get_email_score = self.get_email_score
        test_data["EmailScore"] = test_data.parallel_apply(lambda row: get_email_score(row, self.NO_INTERVAL), axis=1)
        test_data["EmailScore_ST"] = test_data.parallel_apply(lambda row: get_email_score(row, self.SHORT_TERM), axis=1)
        test_data["EmailScore_MT"] = test_data.parallel_apply(lambda row: get_email_score(row, self.MEDIUM_TERM),
                                                              axis=1)
        test_data["EmailScore_LT"] = test_data.parallel_apply(lambda row: get_email_score(row, self.LONG_TERM), axis=1)
        print(str(datetime.now()), ": Finished computing Email features...")

        # add four scores for card holder (four levels of decay)
        get_ch_score = self.get_ch_score
        test_data["CHScore"] = test_data.parallel_apply(lambda row: get_ch_score(row, self.NO_INTERVAL), axis=1)
        test_data["CHScore_ST"] = test_data.parallel_apply(lambda row: get_ch_score(row, self.SHORT_TERM), axis=1)
        test_data["CHScore_MT"] = test_data.parallel_apply(lambda row: get_ch_score(row, self.MEDIUM_TERM), axis=1)
        test_data["CHScore_LT"] = test_data.parallel_apply(lambda row: get_ch_score(row, self.LONG_TERM), axis=1)
        print(str(datetime.now()), ": Finished computing Card Holder features...")

        # add four scores for merchant (four levels of decay)
        get_mer_score = self.get_mer_score
        test_data["MerScore"] = test_data.parallel_apply(lambda row: get_mer_score(row, self.NO_INTERVAL), axis=1)
        test_data["MerScore_ST"] = test_data.parallel_apply(lambda row: get_mer_score(row, self.SHORT_TERM), axis=1)
        test_data["MerScore_MT"] = test_data.parallel_apply(lambda row: get_mer_score(row, self.MEDIUM_TERM), axis=1)
        test_data["MerScore_LT"] = test_data.parallel_apply(lambda row: get_mer_score(row, self.LONG_TERM), axis=1)
        print(str(datetime.now()), ": Finished computing Merchant features...")

        # make sure all our A^{tri} matrices have sorted indices (important before computing transaction scores)
        self.A_tri.sort_indices()
        self.A_tri_short.sort_indices()
        self.A_tri_medium.sort_indices()
        self.A_tri_long.sort_indices()

        # add four scores for transaction (four levels of decay)
        compute_trx_score = self.compute_trx_score
        test_data["TrxScore"] = test_data.parallel_apply(lambda row: compute_trx_score(row, self.NO_INTERVAL), axis=1)
        test_data["TrxScore_ST"] = test_data.parallel_apply(lambda row: compute_trx_score(row, self.SHORT_TERM), axis=1)
        test_data["TrxScore_MT"] = test_data.parallel_apply(lambda row: compute_trx_score(row, self.MEDIUM_TERM),
                                                            axis=1)
        test_data["TrxScore_LT"] = test_data.parallel_apply(lambda row: compute_trx_score(row, self.LONG_TERM), axis=1)
        print(str(datetime.now()), ": Finished computing Transaction features...")

    def compute_trx_score(self, row, interval):
        """
        Computes transaction score for a given row from test data. Assumes card holder, merchant, ip and email scores
        have already been computed and added

        :param row:
        :param interval:
        :return:
        """

        card = self.get_card_idx(row[self.card_col_name])
        merchant = self.get_merchant_idx(row[self.merchant_col_name])
        ip = self.get_ip_idx(row[self.ip_col_name])
        email = self.get_email_idx(row[self.email_col_name])

        if card < self.num_c and merchant < self.num_m and ip < self.num_ip and email < self.num_e:
            # card ID. merchant ID. IP, Email all appear in training data, so it's possible we'll find a transaction
            # among them in training data

            # get the row in undecayed adjacency matrix with all previous transactions of this card ID
            card_t_row = self.A_tri.getrow(self.num_t + card)
            nonzero_rows, nonzero_cols = card_t_row.tocoo().nonzero()

            # if there are multiple transactions between same merchant and same card holder, we want score
            # of the latest transaction. So, we loop through transactions in reverse order
            for nonzero_col in reversed(nonzero_cols):
                # find the merchant corresponding to this transaction (with already matching card ID)
                transaction_col = self.A_tri.getcol(nonzero_col)
                nonzero_rows_2, nonzero_cols2 = transaction_col.tocoo().nonzero()

                # there must be exactly four entries in nonzero_rows2: first for card ID (= card_t_row),
                # second for merchant (which is the row we want)
                # third for ip
                # forth for email
                if len(nonzero_rows_2) != 4:
                    print("ERROR: len(nonzero_rows_2) != 4")

                old_merchant_idx = nonzero_rows_2[1] - self.num_t - self.num_c
                old_ip_idx = nonzero_rows_2[2] - self.num_t - self.num_c - self.num_m
                old_email_idx = nonzero_rows_2[3] - self.num_t - self.num_c - self.num_m - self.num_ip

                if old_merchant_idx == merchant and old_email_idx == email and old_ip_idx == ip:
                    # found a match, use transaction scores of this transaction
                    if interval == self.NO_INTERVAL:
                        return self.rkc[nonzero_col, 0]
                    elif interval == self.SHORT_TERM:
                        return self.rkc_short[nonzero_col, 0]
                    elif interval == self.MEDIUM_TERM:
                        return self.rkc_medium[nonzero_col, 0]
                    elif interval == self.LONG_TERM:
                        return self.rkc_long[nonzero_col, 0]
                    else:
                        print("ERROR: unknown interval type!")

        # couldn't find transaction between same merchant and card holder in training data
        # so, use the ''local update'' rule to compute score for transaction
        merchant_influence = 0
        card_influence = 0
        ip_influence = 0
        email_influence = 0

        if card < self.num_c:
            card_col = self.num_t + card

            if interval == self.NO_INTERVAL:
                card_influence = (row["CHScore"] / (self.A_tri.getcol(card_col).sum() + 1))
            elif interval == self.SHORT_TERM:
                card_influence = (row["CHScore_ST"] / (self.A_tri_short.getcol(card_col).sum() + 1))
            elif interval == self.MEDIUM_TERM:
                card_influence = (row["CHScore_MT"] / (self.A_tri_medium.getcol(card_col).sum() + 1))
            elif interval == self.LONG_TERM:
                card_influence = (row["CHScore_LT"] / (self.A_tri_long.getcol(card_col).sum() + 1))
            else:
                print("ERROR: unknown interval type!")
        else:
            # don't have entries for this card in matrix, use average of all cards instead
            if interval == self.NO_INTERVAL:
                if self.A_tri_avg_card is None:
                    # need to compute avg
                    total = 0
                    for c in range(self.num_c):
                        total += self.A_tri.getcol(self.num_t + c).sum()
                    self.A_tri_avg_card = total / self.num_c

                card_influence = (row["CHScore"] / (self.A_tri_avg_card + 1))

            elif interval == self.SHORT_TERM:
                if self.A_tri_short_avg_card is None:
                    # need to compute avg
                    total = 0
                    for c in range(self.num_c):
                        total += self.A_tri_short.getcol(self.num_t + c).sum()
                    self.A_tri_short_avg_card = total / self.num_c

                card_influence = (row["CHScore_ST"] / (self.A_tri_short_avg_card + 1))

            elif interval == self.MEDIUM_TERM:
                if self.A_tri_medium_avg_card is None:
                    # need to compute avg
                    total = 0
                    for c in range(self.num_c):
                        total += self.A_tri_medium.getcol(self.num_t + c).sum()
                    self.A_tri_medium_avg_card = total / self.num_c

                card_influence = (row["CHScore_MT"] / (self.A_tri_medium_avg_card + 1))

            elif interval == self.LONG_TERM:
                if self.A_tri_long_avg_card is None:
                    # need to compute avg
                    total = 0
                    for c in range(self.num_c):
                        total += self.A_tri_long.getcol(self.num_t + c).sum()
                    self.A_tri_long_avg_card = total / self.num_c

                card_influence = (row["CHScore_LT"] / (self.A_tri_long_avg_card + 1))

            else:
                print("ERROR: unknown interval type!")

        if merchant < self.num_m:
            merchant_col = self.num_t + self.num_c + merchant

            if interval == self.NO_INTERVAL:
                merchant_influence = (row["MerScore"] / (self.A_tri.getcol(merchant_col).sum() + 1))
            elif interval == self.SHORT_TERM:
                merchant_influence = (row["MerScore_ST"] / (self.A_tri_short.getcol(merchant_col).sum() + 1))
            elif interval == self.MEDIUM_TERM:
                merchant_influence = (row["MerScore_MT"] / (self.A_tri_medium.getcol(merchant_col).sum() + 1))
            elif interval == self.LONG_TERM:
                merchant_influence = (row["MerScore_LT"] / (self.A_tri_long.getcol(merchant_col).sum() + 1))
            else:
                print("ERROR: unknown interval type!")
        else:
            # don't have entries for this merchant in matrix, use average of all merchants instead
            if interval == self.NO_INTERVAL:
                if self.A_tri_avg_mer is None:
                    # need to compute avg
                    total = 0
                    for m in range(self.num_m):
                        total += self.A_tri.getcol(self.num_t + self.num_c + m).sum()
                    self.A_tri_avg_mer = total / self.num_m

                merchant_influence = (row["MerScore"] / (self.A_tri_avg_mer + 1))

            elif interval == self.SHORT_TERM:
                if self.A_tri_short_avg_mer is None:
                    # need to compute avg
                    total = 0
                    for m in range(self.num_m):
                        total += self.A_tri_short.getcol(self.num_t + self.num_c + m).sum()
                    self.A_tri_short_avg_mer = total / self.num_m

                merchant_influence = (row["MerScore_ST"] / (self.A_tri_short_avg_mer + 1))

            elif interval == self.MEDIUM_TERM:
                if self.A_tri_medium_avg_mer is None:
                    # need to compute avg
                    total = 0
                    for m in range(self.num_m):
                        total += self.A_tri_medium.getcol(self.num_t + self.num_c + m).sum()
                    self.A_tri_medium_avg_mer = total / self.num_m

                merchant_influence = (row["MerScore_MT"] / (self.A_tri_medium_avg_mer + 1))

            elif interval == self.LONG_TERM:
                if self.A_tri_long_avg_mer is None:
                    # need to compute avg
                    total = 0
                    for m in range(self.num_m):
                        total += self.A_tri_long.getcol(self.num_t + self.num_c + m).sum()
                    self.A_tri_long_avg_mer = total / self.num_m

                merchant_influence = (row["MerScore_LT"] / (self.A_tri_long_avg_mer + 1))

            else:
                print("ERROR: unknown interval type!")

        if ip < self.num_ip:
            ip_col = self.num_t + self.num_c + self.num_m + ip

            if interval == self.NO_INTERVAL:
                ip_influence = (row["IPScore"] / (self.A_tri.getcol(ip_col).sum() + 1))
            elif interval == self.SHORT_TERM:
                ip_influence = (row["IPScore_ST"] / (self.A_tri_short.getcol(ip_col).sum() + 1))
            elif interval == self.MEDIUM_TERM:
                ip_influence = (row["IPScore_MT"] / (self.A_tri_medium.getcol(ip_col).sum() + 1))
            elif interval == self.LONG_TERM:
                ip_influence = (row["IPScore_LT"] / (self.A_tri_long.getcol(ip_col).sum() + 1))
            else:
                print("ERROR: unknown interval type!")
        else:
            # don't have entries for this ip in matrix, use average of all ips instead
            if interval == self.NO_INTERVAL:
                if self.A_tri_avg_ip is None:
                    # need to compute avg
                    total = 0
                    for p in range(self.num_ip):
                        total += self.A_tri.getcol(self.num_t + self.num_c + self.num_m + p).sum()
                    self.A_tri_avg_ip = total / self.num_ip

                ip_influence = (row["IPScore"] / (self.A_tri_avg_ip + 1))

            elif interval == self.SHORT_TERM:
                if self.A_tri_short_avg_ip is None:
                    # need to compute avg
                    total = 0
                    for p in range(self.num_ip):
                        total += self.A_tri_short.getcol(self.num_t + self.num_c + self.num_m + p).sum()
                    self.A_tri_short_avg_ip = total / self.num_ip

                ip_influence = (row["IPScore_ST"] / (self.A_tri_short_avg_ip + 1))

            elif interval == self.MEDIUM_TERM:
                if self.A_tri_medium_avg_ip is None:
                    # need to compute avg
                    total = 0
                    for p in range(self.num_ip):
                        total += self.A_tri_medium.getcol(self.num_t + self.num_c + self.num_m + p).sum()
                    self.A_tri_medium_avg_ip = total / self.num_ip

                ip_influence = (row["IPScore_MT"] / (self.A_tri_medium_avg_ip + 1))

            elif interval == self.LONG_TERM:
                if self.A_tri_long_avg_ip is None:
                    # need to compute avg
                    total = 0
                    for p in range(self.num_ip):
                        total += self.A_tri_long.getcol(self.num_t + self.num_c + self.num_m + p).sum()
                    self.A_tri_long_avg_ip = total / self.num_ip

                ip_influence = (row["IPScore_LT"] / (self.A_tri_long_avg_ip + 1))

            else:
                print("ERROR: unknown interval type!")

        if email < self.num_e:
            email_col = self.num_t + self.num_c + self.num_m + self.num_ip + email

            if interval == self.NO_INTERVAL:
                ip_influence = (row["EmailScore"] / (self.A_tri.getcol(email_col).sum() + 1))
            elif interval == self.SHORT_TERM:
                ip_influence = (row["EmailScore_ST"] / (self.A_tri_short.getcol(email_col).sum() + 1))
            elif interval == self.MEDIUM_TERM:
                ip_influence = (row["EmailScore_MT"] / (self.A_tri_medium.getcol(email_col).sum() + 1))
            elif interval == self.LONG_TERM:
                ip_influence = (row["EmailScore_LT"] / (self.A_tri_long.getcol(email_col).sum() + 1))
            else:
                print("ERROR: unknown interval type!")
        else:
            # don't have entries for this ip in matrix, use average of all emails instead
            if interval == self.NO_INTERVAL:
                if self.A_tri_avg_email is None:
                    # need to compute avg
                    total = 0
                    for e in range(self.num_e):
                        total += self.A_tri.getcol(self.num_t + self.num_c + self.num_m + self.num_ip + e).sum()
                    self.A_tri_avg_email = total / self.num_e

                ip_influence = (row["EmailScore"] / (self.A_tri_avg_email + 1))

            elif interval == self.SHORT_TERM:
                if self.A_tri_short_avg_email is None:
                    # need to compute avg
                    total = 0
                    for e in range(self.num_e):
                        total += self.A_tri_short.getcol(self.num_t + self.num_c + self.num_m + self.num_ip + e).sum()
                    self.A_tri_short_avg_email = total / self.num_e

                ip_influence = (row["EmailScore_ST"] / (self.A_tri_short_avg_email + 1))

            elif interval == self.MEDIUM_TERM:
                if self.A_tri_medium_avg_email is None:
                    # need to compute avg
                    total = 0
                    for e in range(self.num_e):
                        total += self.A_tri_medium.getcol(self.num_t + self.num_c + self.num_m + self.num_ip + e).sum()
                    self.A_tri_medium_avg_email = total / self.num_e

                ip_influence = (row["EmailScore_MT"] / (self.A_tri_medium_avg_email + 1))

            elif interval == self.LONG_TERM:
                if self.A_tri_long_avg_email is None:
                    # need to compute avg
                    total = 0
                    for e in range(self.num_e):
                        total += self.A_tri_long.getcol(self.num_t + self.num_c + self.num_m + self.num_ip + e).sum()
                    self.A_tri_long_avg_email = total / self.num_e

                ip_influence = (row["EmailScore_LT"] / (self.A_tri_long_avg_email + 1))

            else:
                print("ERROR: unknown interval type!")

        return merchant_influence + card_influence + ip_influence + email_influence

    def compute_A_entry(self, transaction_date, test_date, gamma, interval):
        """
        Computes a correctly decayed entry for an Adjacency Matrix.
        We use exponential function here, as x is always negative, the output would be always between 0 and 1.
        We use the output to measure the recency of the transaction

        :param transaction_date:
            The date of the transaction corresponding to the matrix entry
        :param test_date:
            We'll decay with respect to this date (latest date from trainingset)
        :param gamma:
            Decay parameter
        :param interval:
            "short", "medium", or "long"
        :return:
            The value for the matrix entry
        """
        timedelta = test_date - transaction_date

        t = 0
        if interval == self.SHORT_TERM:
            # we only care extremely recent transactions in this case
            t = timedelta.total_seconds() / 60.0  # how many minutes between lastest date and transaction date
        elif interval == self.MEDIUM_TERM:
            t = timedelta.total_seconds() / 3600.0  # how many hours between lastest date and transaction date
        elif interval == self.LONG_TERM:
            # we care recent transactions in several days
            t = timedelta.days  # # how many days between latest date and transaction date
        else:
            print("WARNING: Unknown interval in compute_A_entry(): ", interval)

        t /= 100.0  # we want longer time windows for our data

        # just a sanity check, we should have ''reverse decay'' if the given test_date is earlier
        # than some transactions in the training data
        if t < 0:
            t = 0

        # the decay as described in papers can easily lead to some extremely low values, causing numerical
        # instability later on. So we'll prevent it from decaying too far with the max()
        return max(math.exp(-gamma * t), 0.00000001)

    def converge(self, r0, A_tri, alpha, convergence_threshold):
        """
        Runs the convergence procedure to compute the final vector of risk scores

        :param r0:
            Starting vector of scores, which should simply contain decayed entries for all fraudulent transactions
            (and be normalized to sum up to 1 according to first paper, but not explicitly mentioned in second paper)
        :param A_tri:
            Adjacency matrix
        :param alpha:
            Alpha parameter
        :param convergence_threshold:
            Threshold to determine whether or not we reached convergence
        :return:
            The final, converged vector of risk scores
        """

        # following second paper (not first paper), we need to transpose A_tri (at this point in time, actually
        # called P in second paper)
        A_tri = A_tri.copy().transpose()
        r = r0.copy()

        while True:
            r_old = r
            r_new = alpha * (A_tri * r) + (1.0 - alpha) * r0

            max__abs_change = max((r_new - r_old).max(), (r_old - r_new).max())

            r = r_new

            if max__abs_change < convergence_threshold:
                # converged
                break

        return r

    def get_ch_score(self, row, interval):
        """
        Returns the score for the card holder

        :param row:
            row from test data frame
        :param interval:
            "", "short", "medium", or "long" (to specify level of decay)
        :return:
            Risk score for Card Holder
        """

        card = self.get_card_idx(row[self.card_col_name])

        if card >= self.num_c:
            # this card ID doesn't appear in training data at all, use average of all cards instead
            if interval == self.NO_INTERVAL:
                if self.rkc_avg_card is None:
                    # need to compute avg first
                    total = 0
                    for c in range(self.num_c):
                        total += self.rkc[self.num_t + c, 0]
                    self.rkc_avg_card = total / self.num_c

                return self.rkc_avg_card

            elif interval == self.SHORT_TERM:
                if self.rkc_short_avg_card is None:
                    # need to compute avg first
                    total = 0
                    for c in range(self.num_c):
                        total += self.rkc_short[self.num_t + c, 0]
                    self.rkc_short_avg_card = total / self.num_c

                return self.rkc_short_avg_card

            elif interval == self.MEDIUM_TERM:
                if self.rkc_medium_avg_card is None:
                    # need to compute avg first
                    total = 0
                    for c in range(self.num_c):
                        total += self.rkc_medium[self.num_t + c, 0]
                    self.rkc_medium_avg_card = total / self.num_c

                return self.rkc_medium_avg_card

            elif interval == self.LONG_TERM:
                if self.rkc_long_avg_card is None:
                    # need to compute avg first
                    total = 0
                    for c in range(self.num_c):
                        total += self.rkc_long[self.num_t + c, 0]
                    self.rkc_long_avg_card = total / self.num_c

                return self.rkc_long_avg_card

        elif interval == self.NO_INTERVAL:
            return self.rkc[self.num_t + card, 0]
        elif interval == self.SHORT_TERM:
            return self.rkc_short[self.num_t + card, 0]
        elif interval == self.MEDIUM_TERM:
            return self.rkc_medium[self.num_t + card, 0]
        elif interval == self.LONG_TERM:
            return self.rkc_long[self.num_t + card, 0]

    def get_mer_score(self, row, interval):
        """
        Returns the score for the merchant

        :param row:
            row from test data frame
        :param interval:
            "", "short", "medium", or "long" (to specify level of decay)
        :return:
            Risk score for Merchant
        """

        merchant = self.get_merchant_idx(row[self.merchant_col_name])

        if merchant >= self.num_m:
            # this merchant ID doesn't appear in training data at all, use average of all merchants instead
            if interval == self.NO_INTERVAL:
                if self.rkc_avg_mer is None:
                    # need to compute avg first
                    total = 0
                    for m in range(self.num_m):
                        total += self.rkc[self.num_t + self.num_c + m, 0]
                    self.rkc_avg_mer = total / self.num_m

                return self.rkc_avg_mer

            elif interval == self.SHORT_TERM:
                if self.rkc_short_avg_mer is None:
                    # need to compute avg first
                    total = 0
                    for m in range(self.num_m):
                        total += self.rkc_short[self.num_t + self.num_c + m, 0]
                    self.rkc_short_avg_mer = total / self.num_m

                return self.rkc_short_avg_mer

            elif interval == self.MEDIUM_TERM:
                if self.rkc_medium_avg_mer is None:
                    # need to compute avg first
                    total = 0
                    for m in range(self.num_m):
                        total += self.rkc_medium[self.num_t + self.num_c + m, 0]
                    self.rkc_medium_avg_mer = total / self.num_m

                return self.rkc_medium_avg_mer

            elif interval == self.LONG_TERM:
                if self.rkc_long_avg_mer is None:
                    # need to compute avg first
                    total = 0
                    for m in range(self.num_m):
                        total += self.rkc_long[self.num_t + self.num_c + m, 0]
                    self.rkc_long_avg_mer = total / self.num_m

                return self.rkc_long_avg_mer

        elif interval == self.NO_INTERVAL:
            return self.rkc[self.num_t + self.num_c + merchant, 0]
        elif interval == self.SHORT_TERM:
            return self.rkc_short[self.num_t + self.num_c + merchant, 0]
        elif interval == self.MEDIUM_TERM:
            return self.rkc_medium[self.num_t + self.num_c + merchant, 0]
        elif interval == self.LONG_TERM:
            return self.rkc_long[self.num_t + self.num_c + merchant, 0]

    def get_ip_score(self, row, interval):
        """
        Returns the score for the ip

        :param row:
            row from test data frame
        :param interval:
            "", "short", "medium", or "long" (to specify level of decay)
        :return:
            Risk score for ip
        """

        ip = self.get_ip_idx(row[self.ip_col_name])

        if ip >= self.num_ip:
            # this email ID doesn't appear in training data at all, use average of all email instead
            if interval == self.NO_INTERVAL:
                if self.rkc_avg_ip is None:
                    # need to compute avg first
                    total = 0
                    for p in range(self.num_ip):
                        total += self.rkc[self.num_t + self.num_c + self.num_m + p, 0]
                    self.rkc_avg_ip = total / self.num_ip

                return self.rkc_avg_ip

            elif interval == self.SHORT_TERM:
                if self.rkc_short_avg_ip is None:
                    # need to compute avg first
                    total = 0
                    for p in range(self.num_ip):
                        total += self.rkc_short[self.num_t + self.num_c + self.num_m + p, 0]
                    self.rkc_short_avg_ip = total / self.num_ip

                return self.rkc_short_avg_ip

            elif interval == self.MEDIUM_TERM:
                if self.rkc_medium_avg_ip is None:
                    # need to compute avg first
                    total = 0
                    for p in range(self.num_ip):
                        total += self.rkc_medium[self.num_t + self.num_c + self.num_m + p, 0]
                    self.rkc_medium_avg_ip = total / self.num_ip

                return self.rkc_medium_avg_ip

            elif interval == self.LONG_TERM:
                if self.rkc_long_avg_ip is None:
                    # need to compute avg first
                    total = 0
                    for p in range(self.num_ip):
                        total += self.rkc_long[self.num_t + self.num_c + self.num_m + p, 0]
                    self.rkc_long_avg_ip = total / self.num_ip

                return self.rkc_long_avg_ip

        elif interval == self.NO_INTERVAL:
            return self.rkc[self.num_t + self.num_c + self.num_m + ip, 0]
        elif interval == self.SHORT_TERM:
            return self.rkc_short[self.num_t + self.num_c + self.num_m + ip, 0]
        elif interval == self.MEDIUM_TERM:
            return self.rkc_medium[self.num_t + self.num_c + self.num_m + ip, 0]
        elif interval == self.LONG_TERM:
            return self.rkc_long[self.num_t + self.num_c + self.num_m + ip, 0]

    def get_email_score(self, row, interval):
        """
        Returns the score for the email

        :param row:
            row from test data frame
        :param interval:
            "", "short", "medium", or "long" (to specify level of decay)
        :return:
            Risk score for email
        """

        email = self.get_email_idx(row[self.email_col_name])

        if email >= self.num_e:
            # this email ID doesn't appear in training data at all, use average of all email instead
            if interval == self.NO_INTERVAL:
                if self.rkc_avg_email is None:
                    # need to compute avg first
                    total = 0
                    for e in range(self.num_e):
                        total += self.rkc[self.num_t + self.num_c + self.num_m + self.num_ip + e, 0]
                    self.rkc_avg_email = total / self.num_e

                return self.rkc_avg_email

            elif interval == self.SHORT_TERM:
                if self.rkc_short_avg_email is None:
                    # need to compute avg first
                    total = 0
                    for e in range(self.num_e):
                        total += self.rkc_short[self.num_t + self.num_c + self.num_m + self.num_ip + e, 0]
                    self.rkc_short_avg_email = total / self.num_e

                return self.rkc_short_avg_email

            elif interval == self.MEDIUM_TERM:
                if self.rkc_medium_avg_email is None:
                    # need to compute avg first
                    total = 0
                    for e in range(self.num_e):
                        total += self.rkc_medium[self.num_t + self.num_c + self.num_m + self.num_ip + e, 0]
                    self.rkc_medium_avg_email = total / self.num_e

                return self.rkc_medium_avg_email

            elif interval == self.LONG_TERM:
                if self.rkc_long_avg_email is None:
                    # need to compute avg first
                    total = 0
                    for e in range(self.num_e):
                        total += self.rkc_long[self.num_t + self.num_c + self.num_m + self.num_ip + e, 0]
                    self.rkc_long_avg_email = total / self.num_e

                return self.rkc_long_avg_email

        elif interval == self.NO_INTERVAL:
            return self.rkc[self.num_t + self.num_c + self.num_m + self.num_ip + email, 0]
        elif interval == self.SHORT_TERM:
            return self.rkc_short[self.num_t + self.num_c + self.num_m + self.num_ip + email, 0]
        elif interval == self.MEDIUM_TERM:
            return self.rkc_medium[self.num_t + self.num_c + self.num_m + self.num_ip + email, 0]
        elif interval == self.LONG_TERM:
            return self.rkc_long[self.num_t + self.num_c + self.num_m + self.num_ip + email, 0]

    def normalize_rows(self, matrix):
        """
        Normalizes the rows of the given matrix, such that every row sums up to 1

        :param matrix:
            Matrix of which the rows should be normalized
        :return:
            Row-normalized version of the given matrix
        """

        # We'll normalize the row to sum up to 1 by pre-multiplying by a matrix
        # that contains the scalars for every row on the diagonal
        scalars = [1.0 / matrix.getrow(i).sum() for i in range(matrix.shape[0])]

        return diags(scalars, 0) * matrix

    def get_card_idx(self, card_id):
        """
        Helper function which returns (and generates if necessary) an index (which can be used
        in matrices/vectors) for the given Card ID

        :param card_id:
            Card ID
        :return:
            Unique index (will be 0 for first Card ID observed, 1 for the next, etc.)
        """
        if card_id not in self.cards_dict:
            self.cards_dict[card_id] = self.next_card_idx
            self.next_card_idx += 1

        return self.cards_dict[card_id]

    def get_merchant_idx(self, merchant_id):
        """
        Helper function which returns (and generates if necessary) an index (which can be used
        in matrices/vectors) for the given Merchant ID

        :param merchant_id:
            Merchant ID
        :return:
            Unique index (will be 0 for first Merchant ID observed, 1 for the next, etc.)
        """
        if merchant_id not in self.merchants_dict:
            self.merchants_dict[merchant_id] = self.next_merchant_idx
            self.next_merchant_idx += 1

        return self.merchants_dict[merchant_id]

    def get_email_idx(self, email_id):
        """
        Helper function which returns (and generates if necessary) an index (which can be used
        in matrices/vectors) for the given Merchant ID

        :param email_id:
            Email ID
        :return:
            Unique index (will be 0 for first Email ID observed, 1 for the next, etc.)
        """
        if email_id not in self.email_dict:
            self.email_dict[email_id] = self.next_email_idx
            self.next_email_idx += 1

        return self.email_dict[email_id]

    def get_ip_idx(self, ip_id):
        """
        Helper function which returns (and generates if necessary) an index (which can be used
        in matrices/vectors) for the given Merchant ID

        :param ip_id:
            IP ID
        :return:
            Unique index (will be 0 for first IP ID observed, 1 for the next, etc.)
        """
        if ip_id not in self.ip_dict:
            self.ip_dict[ip_id] = self.next_ip_idx
            self.next_ip_idx += 1

        return self.ip_dict[ip_id]
